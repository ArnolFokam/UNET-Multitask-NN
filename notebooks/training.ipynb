{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1138b27",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from configparser import ConfigParser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append('../')\n",
    "from models.metrics import dice_loss, hamming_score\n",
    "from models.pnsamp_2d import PNSAMP_2D\n",
    "from utils.directory import check_or_create\n",
    "from utils.data_generators import DicomDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1c7e0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ConfigParser()\n",
    "parser.read('../project.conf')\n",
    "\n",
    "# Get Directory setting\n",
    "# add '../' since you are in the a notebook\n",
    "saved_weights_path = check_or_create('../' + parser.get('train', 'SAVED_WEIGHTS_PATH'))\n",
    "checkpoints_path = check_or_create('../' + parser.get('train', 'CHECKPOINTS_PATH'))\n",
    "history_path = check_or_create('../' + parser.get('train', 'HISTORY_PATH'))\n",
    "data_path = check_or_create('../' + parser.get('train', 'DATA_PATH'))\n",
    "batch_size = int(parser.get('train', 'BATCH_SIZE'))\n",
    "image_size = int(parser.get('train', 'IMAGE_SIZE'))\n",
    "variant = parser.get('train', 'VARIANT')\n",
    "epochs = int(parser.get('train', 'EPOCHS'))\n",
    "test_ratio = float(parser.get('train', 'TEST_RATIO'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_path, 'meta/meta_info.csv'),\n",
    "                         dtype={'patient_id': str,\n",
    "                                'nodule_no': str,\n",
    "                                'slice_no': str})\n",
    "\n",
    "# use only non-clean scans (scans that contains at least one nodule) for training\n",
    "df = df[df['is_clean'] == False]\n",
    "\n",
    "def get_paths(x):\n",
    "    patient_img_path = os.path.join(data_path, 'image', 'LIDC-IDRI-' + x[0])\n",
    "    patient_mask_path = os.path.join(data_path, 'mask', 'LIDC-IDRI-' + x[0])\n",
    "    return [os.path.join(patient_img_path, x[1] + '.npy'), os.path.join(patient_mask_path, x[2] + '.npy')]\n",
    "\n",
    "temp = df[['patient_id',\n",
    "                'original_image',\n",
    "                'mask_image']].values\n",
    "\n",
    "paths = list(map(get_paths, temp))\n",
    "df_paths = pd.DataFrame(paths, columns=['img_path', 'mask_path'])\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_paths.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = pd.concat([df, df_paths], axis=1, sort=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['subtlety',\n",
    "            'margin',\n",
    "            'lobulation',\n",
    "            'texture',]\n",
    "\n",
    "\"\"\"from sklearn.preprocessing import MinMaxScaler\n",
    "df[features] = MinMaxScaler().fit_transform(df[features])\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cfbe6e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_batches = int(len(df) / batch_size)\n",
    "\n",
    "print(\"|=============BUILDING GENERATOR=============|\\n\")\n",
    "datagen = DicomDataGenerator(df,\n",
    "                             img_path_col_name='img_path',\n",
    "                             mask_path_col_name='mask_path',\n",
    "                             features_cols=features,\n",
    "                             batch_size=batch_size,\n",
    "                             target_size=(image_size, image_size, 1))\n",
    "\n",
    "traingen, valigen = train_test_split(datagen, test_size=test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PNSAMP_2D(num_attributes=len(features), input_size=(image_size, image_size, 1), variant=variant)\n",
    "model.summary()\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Instantiate a loss function.\n",
    "# loss_fn1 = tf.keras.losses.BinaryCrossentropy()\n",
    "loss_fn1 = dice_loss\n",
    "loss_fn2 = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "loss_fn3 = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# setup training checkpoints\n",
    "checkpoint = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(\n",
    "            checkpoint,\n",
    "            os.path.join(checkpoints_path, variant),\n",
    "            max_to_keep=3\n",
    "        )\n",
    "\n",
    "checkpoint.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Note: Starting training with model restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Note: Starting training from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a7389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# record of training\n",
    "history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'training_time': [],\n",
    "    'train_acc': [],\n",
    "            'train_ham': [],\n",
    "            'val_acc': [],\n",
    "            'val_ham': [],\n",
    "        }\n",
    "\n",
    "# the validation will be use save a\n",
    "# checkpoint of the model for the\n",
    "# best loss\n",
    "best_validation_loss = np.inf\n",
    "\n",
    "print(\"|==================TRAINING==================|\\n\")\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    total_loss1 = 0.0\n",
    "    total_loss2 = 0.0\n",
    "    total_loss3 = 0.0\n",
    "    total_acc_mal = 0.0\n",
    "    total_hamming = 0.0\n",
    "    start_train_time = time.time()\n",
    "    i = 0\n",
    "    # Iterate over the batches of the train dataset.\n",
    "    for _, (x_batch_train, mask_batch_train, feats_batch_train, mal_batch_train) in enumerate(traingen):\n",
    "        i += 1\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Run the forward pass of the layer. The operations that the layer applies to its inputs are\n",
    "            # going to be recorded on the GradientTape. segmentation_logits, multi_regr_logits, class_logits\n",
    "            segmentation_logits, multi_regr_logits, class_logits = model(x_batch_train, training=True)\n",
    "            \n",
    "            # Logits for this minibatch\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value1 = loss_fn1(mask_batch_train, segmentation_logits)\n",
    "            loss_value2 = loss_fn2(feats_batch_train, multi_regr_logits)\n",
    "            loss_value3 = loss_fn3(mal_batch_train, class_logits)\n",
    "            print(loss_value1.numpy(), loss_value2.numpy(), loss_value3.numpy())\n",
    "            \n",
    "            \n",
    "            # add different losses to total loss\n",
    "            total_loss1 += loss_value1.numpy()\n",
    "            total_loss2 += loss_value1.numpy()\n",
    "            total_loss3 += loss_value1.numpy()\n",
    "            total_acc_mal += accuracy_score(\n",
    "                mal_batch_train.numpy(),\n",
    "                tf.math.argmax(class_logits, axis=-1).numpy())\n",
    "            total_hamming += hamming_score(\n",
    "                feats_batch_train.numpy(),\n",
    "                tf.math.argmax(multi_regr_logits, axis=-1).numpy())\n",
    "\n",
    "        \"\"\"MULTI-CLASSIFICATOIN\"\"\"\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        # grads = tape.gradient(loss_value1, model.trainable_weights)\n",
    "        grads = tape.gradient([loss_value1, loss_value2, loss_value3], model.trainable_weights)\n",
    "\n",
    "        optimizer.apply_gradients(\n",
    "            (grad, var) \n",
    "            for (grad, var) in zip(grads, model.trainable_variables) \n",
    "            if grad is not None\n",
    "        )\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        # optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    end_train_time = time.time()\n",
    "    history['training_time'].append(end_train_time - start_train_time)\n",
    "    history['train_loss'].append(total_loss1 / i)\n",
    "    history['train_acc'].append(total_acc_mal / i)\n",
    "    history['train_ham'].append(total_hamming / i)\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    total_loss1 = 0.0\n",
    "    total_loss2 = 0.0\n",
    "    total_loss3 = 0.0\n",
    "    total_acc_mal = 0.0\n",
    "    total_hamming = 0.0\n",
    "    start_train_time = time.time()\n",
    "    i = 0\n",
    "    for _, (x_batch_val, mask_batch_val, feats_batch_val, mal_batch_val) in enumerate(valigen):\n",
    "        i += 1\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            segmentation_logits, multi_regr_logits, class_logits = model(x_batch_val, training=True)\n",
    "\n",
    "            # Compute the loss value for this mini batch.\n",
    "            loss_value1 = loss_fn1(mask_batch_val, segmentation_logits)\n",
    "            loss_value2 = loss_fn2(feats_batch_val, multi_regr_logits)\n",
    "            loss_value3 = loss_fn3(mal_batch_val, class_logits)\n",
    "            print(loss_value1.numpy(), loss_value2.numpy(), loss_value3.numpy())\n",
    "\n",
    "        # add different losses to total loss\n",
    "        total_loss1 += loss_value1.numpy()\n",
    "        total_loss2 += loss_value1.numpy()\n",
    "        total_loss3 += loss_value1.numpy()\n",
    "        total_acc_mal += accuracy_score(\n",
    "            mal_batch_val.numpy(),\n",
    "            tf.math.argmax(class_logits, axis=-1).numpy())\n",
    "        total_hamming += hamming_score(\n",
    "            feats_batch_train.numpy(),\n",
    "            tf.math.argmax(multi_regr_logits, axis=-1).numpy())\n",
    "\n",
    "    history['val_loss'].append(total_loss1 / i)\n",
    "    history['val_acc'].append(total_acc_mal / i)\n",
    "    history['val_ham'].append(total_hamming / i)\n",
    "\n",
    "    # save model if performance is better (loss is lower)\n",
    "    if history['val_loss'][-1] < best_validation_loss:\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for step {}: {}, loss {:1.3f}\".format(int(checkpoint.step),\n",
    "                                                                              save_path,\n",
    "                                                                              history['val_loss'][-1]))\n",
    "        model.save_weights(os.path.join(saved_weights_path, variant), save_format='tf')\n",
    "        checkpoint.step.assign_add(1)\n",
    "        best_validation_loss = history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff028df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final training loss', history['train_loss'][-1])\n",
    "print('Final validation loss', history['val_loss'][-1])\n",
    "\n",
    "# save history file\n",
    "today = datetime.datetime.now()\n",
    "\n",
    "if today.hour < 12:\n",
    "    h = \"00\"\n",
    "else:\n",
    "    h = \"12\"\n",
    "    \n",
    "file_path = check_or_create(os.path.join(history_path, variant))\n",
    "\n",
    "with open(os.path.join(file_path, 'history_{}.json'.format(today.strftime('%Y%m%d') + h +\n",
    "                                                                                 str(today.minute))),'w') as fp:\n",
    "    json.dump(history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710e9b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
